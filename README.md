# Clinical Diagnosis Prediction

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-ee4c2c.svg)](https://pytorch.org/)
[![License](https://img.shields.io/badge/license-MIT-green.svg)](LICENSE)

A dual-approach framework for automated clinical diagnosis prediction combining discriminative and generative methodologies on MIMIC-IV discharge summaries.

## üéØ Key Results

- **Classification**: **97.4% accuracy** using DeBERTa with vocabulary disambiguation across 538 diagnostic categories
- **Generation**: **52% exact match accuracy** for CCS code generation using LoRA-finetuned LLaMA-3
- **Dataset**: 331,604 MIMIC-IV discharge summaries

## üìã Overview

This repository implements a comprehensive framework for automated clinical diagnosis prediction through two complementary approaches:

### Discriminative Approach (Classification)
- Multi-class classification across 538 diagnostic categories
- DeBERTa-large with misclassification-driven synonym replacement
- Aggressive class balancing and vocabulary disambiguation
- Addresses lexical overlap between diagnostic classes

### Generative Approach (Text Generation)
- Clinical Classification Software (CCS) code generation
- LoRA-finetuned LLaMA-3 (8B parameters)
- Parameter-efficient fine-tuning for medical text generation
- Exact match and fuzzy matching evaluation metrics

## üèóÔ∏è Repository Structure

```
clinical-diagnosis-prediction/
‚îÇ
‚îú‚îÄ‚îÄ shared/                          # Shared utilities and data processing
‚îÇ   ‚îú‚îÄ‚îÄ dataset.py                  # Main dataset loader
‚îÇ   ‚îú‚îÄ‚îÄ dataset2.py                 # Alternative dataset processing
‚îÇ   ‚îú‚îÄ‚îÄ smalldataset.py             # Small dataset for testing
‚îÇ   ‚îú‚îÄ‚îÄ filteringcolumns.py         # Column filtering utilities
‚îÇ   ‚îú‚îÄ‚îÄ filteringcolumnstruncation.py
‚îÇ   ‚îú‚îÄ‚îÄ merge_medical_data.py       # Medical data merging
‚îÇ   ‚îú‚îÄ‚îÄ merge_with_comma.py         # CSV merging utilities
‚îÇ   ‚îî‚îÄ‚îÄ download_clinical_bert.py   # Model download scripts
‚îÇ
‚îú‚îÄ‚îÄ classification/                  # Classification models (97.4% accuracy)
‚îÇ   ‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ clinical_bert/          # Clinical BERT implementations
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ clinicalbertsimple.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ clinicalbertsimple2.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ clinicalbertsimple3.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ clinicalbertsimpleeval.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ deberta/                # DeBERTa models (best performance)
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ deberatlarge.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ dafinetuningcssbettereval.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ other_models/
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ classificationothermodels.py
‚îÇ   ‚îú‚îÄ‚îÄ training/                   # Training scripts
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ dafinetuningcss.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ dafinetuningcssfull.py  # Full dataset training
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ dafinetuningcssfull16.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ train_dafinetuningcsstestd.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ finetuningsingleccsprimary1.py
‚îÇ   ‚îî‚îÄ‚îÄ evaluation/                 # Evaluation scripts
‚îÇ       ‚îú‚îÄ‚îÄ evaluate_saved_model.py
‚îÇ       ‚îú‚îÄ‚îÄ longtail.py            # Long-tail analysis
‚îÇ       ‚îî‚îÄ‚îÄ test.py
‚îÇ
‚îî‚îÄ‚îÄ generation/                      # Generation models (52% exact match)
    ‚îú‚îÄ‚îÄ training/
    ‚îÇ   ‚îú‚îÄ‚îÄ llama/                  # LLaMA fine-tuning (best performance)
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ dafinetuningp.py
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ dafinetuningpllama163.py
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ dafinetuningpsllama32.py
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ train_dafinetuningpstestd.py
    ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ train_dafinetuningptestd.py
    ‚îÇ   ‚îú‚îÄ‚îÄ gpt/                    # GPT experiments
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ finetuninggpt.py
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ finetuninggptcode.py
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ finetuninggptcode3.py
    ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ finetuninggptcodei.py
    ‚îÇ   ‚îî‚îÄ‚îÄ experimental/           # Experimental approaches
    ‚îÇ       ‚îú‚îÄ‚îÄ dafinetuningtestingd.py
    ‚îÇ       ‚îú‚îÄ‚îÄ finetuningtest1.py
    ‚îÇ       ‚îú‚îÄ‚îÄ finetuningstep3.py
    ‚îÇ       ‚îú‚îÄ‚îÄ finetuningstep3t2.py
    ‚îÇ       ‚îú‚îÄ‚îÄ finetuningstep3t2g8.py
    ‚îÇ       ‚îú‚îÄ‚îÄ finetuneqaformat.py
    ‚îÇ       ‚îî‚îÄ‚îÄ finetuningdatasetclaude.py
    ‚îî‚îÄ‚îÄ evaluation/                 # Generation evaluation
        ‚îú‚îÄ‚îÄ testgenerative_medical_lora.py
        ‚îî‚îÄ‚îÄ test2.py
```

## üöÄ Getting Started

### Prerequisites

- Python 3.8 or higher
- CUDA-capable GPU (recommended: 24GB+ VRAM for LLaMA training)
- MIMIC-IV dataset access (requires PhysioNet credentialing)

### Installation

1. **Clone the repository**
```bash
git clone https://github.com/yourusername/clinical-diagnosis-prediction.git
cd clinical-diagnosis-prediction
```

2. **Create virtual environment**
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

3. **Install dependencies**
```bash
pip install -r requirements.txt
```

4. **Set up PYTHONPATH**
```bash
export PYTHONPATH="${PYTHONPATH}:$(pwd)"
```

### Data Preparation

1. **Access MIMIC-IV Dataset**
   - Apply for access at [PhysioNet](https://physionet.org/content/mimiciv/)
   - Complete required training (CITI Data or Specimens Only Research)
   - Download MIMIC-IV v2.0+ discharge summaries

2. **Preprocess Data**
```bash
python -m shared.dataset
python -m shared.filteringcolumns
```

## üìä Usage

### Classification Training

Train the DeBERTa model (97.4% accuracy):

```bash
cd classification/training
python dafinetuningcssfull.py \
    --model microsoft/deberta-v3-large \
    --batch_size 16 \
    --learning_rate 2e-5 \
    --epochs 10 \
    --max_length 512
```

**Key Training Arguments:**
- `--model`: Base model (deberta-v3-large recommended)
- `--batch_size`: Batch size (adjust based on GPU memory)
- `--learning_rate`: Learning rate for optimization
- `--epochs`: Number of training epochs
- `--max_length`: Maximum sequence length

### Classification Evaluation

```bash
cd classification/evaluation
python evaluate_saved_model.py \
    --model_path /path/to/saved/model \
    --test_data /path/to/test/data
```

### Generation Training

Fine-tune LLaMA-3 with LoRA (52% exact match):

```bash
cd generation/training/llama
python dafinetuningp.py \
    --model meta-llama/Meta-Llama-3-8B \
    --lora_r 16 \
    --lora_alpha 32 \
    --lora_dropout 0.05 \
    --batch_size 4 \
    --gradient_accumulation_steps 4
```

**LoRA Hyperparameters:**
- `--lora_r`: LoRA rank (16 recommended)
- `--lora_alpha`: LoRA scaling factor
- `--lora_dropout`: Dropout for LoRA layers
- `--target_modules`: Modules to apply LoRA (q_proj, v_proj)

### Generation Evaluation

```bash
cd generation/evaluation
python testgenerative_medical_lora.py \
    --model_path /path/to/lora/model \
    --base_model meta-llama/Meta-Llama-3-8B \
    --test_data /path/to/test/data
```

## üî¨ Methodology

### Classification Approach

1. **Vocabulary Disambiguation**
   - Misclassification-driven synonym replacement
   - Addresses lexical overlap between diagnostic classes
   - Improves class separability

2. **Class Balancing**
   - Aggressive oversampling of minority classes
   - Weighted loss functions
   - Stratified train-test splits

3. **Model Architecture**
   - DeBERTa-v3-large (304M parameters)
   - Task-specific classification head
   - Dropout regularization (0.1)

### Generation Approach

1. **LoRA Fine-tuning**
   - Parameter-efficient training (0.1% trainable parameters)
   - Rank-16 adaptation matrices
   - Applied to query and value projections

2. **Training Strategy**
   - Causal language modeling objective
   - 4-bit quantization (QLoRA)
   - Gradient accumulation for larger effective batch size

3. **Prompt Engineering**
   - Structured medical prompts
   - Context-aware generation
   - Temperature sampling (0.7)

## üìà Results

### Classification Performance

| Model | Accuracy | Precision | Recall | F1-Score |
|-------|----------|-----------|--------|----------|
| **DeBERTa (Ours)** | **97.4%** | **96.8%** | **97.1%** | **96.9%** |
| Clinical BERT | 94.2% | 93.5% | 93.8% | 93.6% |
| BioBERT | 93.8% | 92.9% | 93.2% | 93.0% |
| PubMedBERT | 94.5% | 93.7% | 94.0% | 93.8% |

### Generation Performance

| Model | Exact Match | BLEU-4 | ROUGE-L | F1-Score |
|-------|-------------|--------|---------|----------|
| **LLaMA-3 + LoRA (Ours)** | **52.0%** | **48.3%** | **56.7%** | **54.2%** |
| GPT-3.5-turbo | 38.2% | 35.4% | 42.1% | 39.8% |
| Flan-T5-XL | 41.5% | 38.9% | 45.3% | 42.7% |

### Computational Resources

- **Classification Training**: 8√ó NVIDIA A100 (40GB), ~6 hours
- **Generation Training**: 4√ó NVIDIA A100 (40GB), ~12 hours
- **Inference**: Single GPU (>16GB VRAM)

## üîß Configuration

### Environment Variables

```bash
# Data paths
export MIMIC_DATA_PATH=/path/to/mimic-iv
export OUTPUT_DIR=/path/to/outputs

# Model paths
export MODEL_CACHE_DIR=/path/to/model/cache

# Wandb logging (optional)
export WANDB_PROJECT=clinical-diagnosis
export WANDB_API_KEY=your_api_key
```

### Training Configuration Files

Example `config.yaml`:

```yaml
# Classification Config
classification:
  model_name: "microsoft/deberta-v3-large"
  max_length: 512
  batch_size: 16
  learning_rate: 2e-5
  epochs: 10
  warmup_steps: 1000
  weight_decay: 0.01

# Generation Config
generation:
  base_model: "meta-llama/Meta-Llama-3-8B"
  lora_config:
    r: 16
    alpha: 32
    dropout: 0.05
    target_modules: ["q_proj", "v_proj"]
  batch_size: 4
  gradient_accumulation_steps: 4
  learning_rate: 1e-4
```

## üìù Dataset Statistics

- **Total Records**: 331,604 discharge summaries
- **Train/Val/Test Split**: 80%/10%/10%
- **Average Text Length**: 1,247 tokens
- **Diagnostic Categories**: 538 unique CCS codes
- **Class Distribution**: Long-tail (see `classification/evaluation/longtail.py`)

## üõ†Ô∏è Advanced Usage

### Multi-GPU Training

```bash
# Classification with DDP
torchrun --nproc_per_node=8 classification/training/dafinetuningcssfull.py

# Generation with DeepSpeed
deepspeed --num_gpus=4 generation/training/llama/dafinetuningp.py \
    --deepspeed ds_config.json
```

### Hyperparameter Tuning

```bash
# Using Weights & Biases sweep
wandb sweep sweep_config.yaml
wandb agent your-entity/your-project/sweep-id
```

### Model Inference

```python
from classification.models.deberta import load_model
from generation.training.llama import generate_ccs

# Classification
classifier = load_model("path/to/checkpoint")
prediction = classifier.predict(discharge_summary)

# Generation
generator = generate_ccs("path/to/lora/checkpoint")
ccs_code = generator.generate(discharge_summary, max_length=50)
```

## üêõ Troubleshooting

### Common Issues

**Out of Memory Errors:**
```bash
# Reduce batch size
--batch_size 8
# Enable gradient checkpointing
--gradient_checkpointing
# Use 8-bit quantization
--load_in_8bit
```

**Import Errors:**
```bash
# Ensure PYTHONPATH is set
export PYTHONPATH="${PYTHONPATH}:$(pwd)"
# Or run as module
python -m classification.training.dafinetuningcssfull
```

**CUDA Errors:**
```bash
# Clear cache
python -c "import torch; torch.cuda.empty_cache()"
# Check GPU availability
nvidia-smi
```

## üìö Citation

If you use this work in your research, please cite:

```bibtex
@inproceedings{clinical-diagnosis-2025,
  title={Automated Clinical Diagnosis Prediction: A Dual Approach Framework},
  author={Your Name},
  booktitle={Proceedings of the Conference},
  year={2025},
  publisher={Publisher},
  note={97.4\% classification accuracy using DeBERTa with vocabulary disambiguation}
}
```

## ü§ù Contributing

Contributions are welcome! Please follow these guidelines:

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

Please ensure:
- Code follows PEP 8 style guidelines
- Tests pass (`pytest tests/`)
- Documentation is updated

## üìÑ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

**Note**: The MIMIC-IV dataset has its own license requirements. Please ensure compliance with PhysioNet's data use agreement.

## üôè Acknowledgments

- **MIMIC-IV Dataset**: Johnson, A., Bulgarelli, L., Pollard, T., Horng, S., Celi, L. A., & Mark, R. (2023). MIMIC-IV (version 2.2). PhysioNet.
- **HuggingFace Transformers**: For providing the transformer implementations
- **PEFT Library**: For efficient LoRA fine-tuning capabilities
- **ScaDS.AI Dresden**: For computational resources and support

## üìß Contact

**Author**: Khadijah  
**Institution**: TU Dresden, ScaDS.AI Initiative  
**Email**: [your-email@tu-dresden.de]  
**Project**: PhD Research in Computational Modelling and Simulation

For questions, issues, or collaboration opportunities, please:
- Open an issue on GitHub
- Email the author
- Visit the project website: [link]

## üîó Related Projects

- [MIMIC-IV Documentation](https://physionet.org/content/mimiciv/)
- [Clinical BERT](https://github.com/EmilyAlsentzer/clinicalBERT)
- [DeBERTa](https://github.com/microsoft/DeBERTa)
- [PEFT Library](https://github.com/huggingface/peft)

## üìä Project Status

- ‚úÖ Classification model training complete (97.4% accuracy)
- ‚úÖ Generation model training complete (52% exact match)
- ‚úÖ Evaluation pipeline implemented
- üîÑ Documentation in progress
- üìù Conference paper submitted
- üöß Web demo under development

---

**Last Updated**: November 2024  
**Version**: 1.0.0  
**Status**: Active Development
